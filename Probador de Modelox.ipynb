{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a970d0f-82cc-49e7-88c2-1da28f792a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow; print(\"Versi贸n MLFLOW:\", mlflow.__version__)\n",
    "import torch ; print(\"VERSIN TORCH:\", torch.__version__)\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "from itertools import product\n",
    "import pandas as pd ;print(\"VERSIN PANDAS:\", pd.__version__)\n",
    "import gc\n",
    "\n",
    "token = \"TU TOKEN DE HF\"\n",
    "login(token)\n",
    "\n",
    "# Configuraci贸n de MLflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e1fad-6304-4adf-a14e-9cc96bf7393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"deepseek-ai/Deepseek-R1-Distill-Qwen-1.5B\"\n",
    "#model_name = \"ozone-research/Chirp-01\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "#experimento = \"granite-3.2-2b-instruct\"\n",
    "experimento = \"Llama-3.2-3B-Instruct\"\n",
    "mlflow.set_experiment(experimento)\n",
    "experimento_cuantizado = experimento + \"-Cuantizado\"\n",
    "\n",
    "usar_quantizacion = False\n",
    "usar_pipeline = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750427d0-de46-4c6c-abe1-78cf7dbd7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if usar_pipeline:\n",
    "    if usar_quantizacion:\n",
    "        print(\"Usando quantizaci贸n 4-bit con bitsandbytes.\")\n",
    "        mlflow.set_experiment(experimento_cuantizado)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant = True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            trust_remote_code=True, \n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) \n",
    "\n",
    "        text_generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        text_generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    print(f\"El modelo '{model_name}' es compatible con pipeline().\")\n",
    "\n",
    "else:\n",
    "    print(f\"El modelo '{model_name}' NO es compatible con pipeline(). Cargando manualmente.\")\n",
    "\n",
    "    if usar_quantizacion:\n",
    "        print(\" Usando quantizaci贸n 4-bit con bitsandbytes.\")\n",
    "        mlflow.set_experiment(experimento_cuantizado)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ccc0d3f-4825-4a9b-9c77-1c6a3826125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici贸n de queries\n",
    "queries = {\n",
    "    \"en\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a useful AI assistant that extracts possible words related to the user's intent. Keep in mind that the keywords are not only within the given phrase. Present all your ideas without asking me, and delve deeply into the topic. Print only a list of keywords or ideas, without introduction, conclusion, or connectors between ideas. Use a comma separated list.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I want to search information about the wetland in Torrevieja called Parque Natural de las Lagunas de la Mata.\"}\n",
    "    ],\n",
    "    \"es\": [\n",
    "        {'role': 'system', 'content': 'Eres un 煤til asistente de IA que extrae posibles palabras relacionadas con la intenci贸n del usuario. Ten en cuenta que las palabras clave no solo est谩n en la frase. Plasma todas tus ideas sin consultarme, es decir, profundiza en el tema. Imprime 煤nicamente una lista separada por comas con las palabras clave o ideas, nada m谩s, sin introducci贸n ni conclusi贸n ni conectores entre ideas.'}, \n",
    "        {'role': 'user', 'content': 'Quiero buscar informaci贸n sobre un humedal en Torrevieja llamado Parque Natural de las Lagunas de la Mata.'}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e051c97c-974c-4ca0-9f94-c03647d872cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malla de par谩metros\n",
    "temperaturas = [0.4, 0.7, 1.0]\n",
    "top_keys_vals = [20, 40, 60]\n",
    "top_words_vals = [0.85, 0.9, 0.95]\n",
    "num_beams_vals = [1, 2, 4]\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "param_combinations = list(product(temperaturas, top_keys_vals, top_words_vals, num_beams_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3548d-377d-4741-b70f-d2391baf5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci贸n para ejecutar el modelo y registrar en MLflow\n",
    "def ejecutar_experimento(query_name, messages, temp_val, top_k_val, top_p_val, num_beams_val, usar_pipeline, usar_quantizacion):\n",
    "    previous_runs = mlflow.search_runs(order_by=[\"start_time\"])\n",
    "    print(messages)\n",
    "\n",
    "    if \"tags.mlflow.runName\" in previous_runs.columns:\n",
    "        count = sum(previous_runs[\"tags.mlflow.runName\"].fillna(\"\").str.startswith(query_name)) + 1\n",
    "    else:\n",
    "        count = 1  \n",
    "        \n",
    "    run_name = f\"{query_name}-{count}\"  # Nombre en formato idioma-contador\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"mlflow.runName\", run_name)\n",
    "        mlflow.log_param(\"modelo\", model_name)\n",
    "        mlflow.log_param(\"query_idioma\", query_name)\n",
    "        mlflow.log_param(\"temperatura\", temp_val)\n",
    "        mlflow.log_param(\"top_k\", top_k_val)\n",
    "        mlflow.log_param(\"top_p\", top_p_val)\n",
    "        mlflow.log_param(\"num_beams\", num_beams_val)\n",
    "        mlflow.log_param(\"quantizado\", usar_quantizacion)\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        inicio = time.time()\n",
    "\n",
    "        if usar_pipeline:    \n",
    "            output = text_generator(\n",
    "                messages,\n",
    "                max_new_tokens = 500,\n",
    "                do_sample=True,\n",
    "                temperature=temp_val,\n",
    "                top_k=top_k_val,\n",
    "                top_p=top_p_val,\n",
    "                num_beams=num_beams_val,\n",
    "                repetition_penalty=1.2,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            respuesta = output[0][\"generated_text\"]\n",
    "            final = time.time() - inicio\n",
    "\n",
    "            palabras = respuesta.split(\" \")\n",
    "            cantidad_palabras = len(palabras)\n",
    "            \n",
    "            keywords = respuesta.split(\",\")\n",
    "            cantidad_keywords = len(keywords)\n",
    "            mlflow.log_metric(\"palabras\", cantidad_palabras)\n",
    "            mlflow.log_metric(\"palabrasxsegundo\", cantidad_palabras / final)\n",
    "            mlflow.log_metric(\"keywords\", cantidad_keywords)\n",
    "            mlflow.log_metric(\"keywordsxsegundo\", cantidad_keywords / final)\n",
    "        else:\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    input_ids.to(\"cuda\"),\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    max_new_tokens = 500,\n",
    "                    do_sample=True,\n",
    "                    temperature=temp_val,\n",
    "                    top_k=top_k_val,\n",
    "                    top_p=top_p_val,\n",
    "                    num_beams=num_beams_val,\n",
    "                    repetition_penalty=1.2\n",
    "                )\n",
    "    \n",
    "            respuesta = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()\n",
    "            if (query_name == \"en\"):\n",
    "                respuesta = respuesta.split(\"assistant\")[2]\n",
    "            else:\n",
    "                respuesta = respuesta.split(\"assistant\")[1]\n",
    "                \n",
    "            final = time.time() - inicio\n",
    "\n",
    "            palabras = respuesta.split(\" \")\n",
    "            cantidad_palabras = len(palabras)\n",
    "            \n",
    "            keywords = respuesta.split(\",\")\n",
    "            cantidad_keywords = len(keywords)\n",
    "\n",
    "            mlflow.log_metric(\"palabras\", cantidad_palabras)\n",
    "            mlflow.log_metric(\"palabrasxsegundo\", cantidad_palabras / final)\n",
    "            mlflow.log_metric(\"keywords\", cantidad_keywords)\n",
    "            mlflow.log_metric(\"keywordsxsegundo\", cantidad_keywords / final)\n",
    "        \n",
    "        memoria_usada = round(torch.cuda.memory_allocated() / (1024**2), 2)\n",
    "        memoria_max_usada = round(torch.cuda.max_memory_allocated() / (1024**2), 2)\n",
    "\n",
    "        mlflow.set_tag(\"respuesta\", respuesta)\n",
    "        mlflow.log_metric(\"tiempo_ejecucion\", time.time() - inicio)\n",
    "        mlflow.log_metric(\"gpu_mem_usada_MB\", memoria_usada)\n",
    "        mlflow.log_metric(\"gpu_mem_max_usada_MB\", memoria_max_usada)\n",
    "    \n",
    "        print(f\"\\n--- Respuesta ---\\n\", respuesta)\n",
    "        print(\"Tiempo de ejecuci贸n:\", time.time() - inicio)\n",
    "\n",
    "# Ejecutar experimentos en ingl茅s y espa帽ol\n",
    "for idx, (temp_val, top_k_val, top_p_val, num_beams_val) in enumerate(param_combinations):\n",
    "    ejecutar_experimento(\"en\", queries[\"en\"], temp_val, top_k_val, top_p_val, num_beams_val, usar_pipeline, usar_quantizacion)\n",
    "    ejecutar_experimento(\"es\", queries[\"es\"], temp_val, top_k_val, top_p_val, num_beams_val, usar_pipeline, usar_quantizacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af9b18-88f6-4ec0-b7a1-a65c3dad4269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Idioma</th>\n",
       "      <th>Media Tiempo (s)</th>\n",
       "      <th>Media Palabras/s</th>\n",
       "      <th>Media Keywords/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>en</td>\n",
       "      <td>11.420085</td>\n",
       "      <td>14.469464</td>\n",
       "      <td>7.973382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>es</td>\n",
       "      <td>3.275390</td>\n",
       "      <td>9.749098</td>\n",
       "      <td>7.176375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Modelo Idioma  Media Tiempo (s)  Media Palabras/s  Media Keywords/s\n",
       "0  Llama-3.2-3B     en         11.420085         14.469464          7.973382\n",
       "1  Llama-3.2-3B     es          3.275390          9.749098          7.176375"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcular_estadisticas_experimentos():\n",
    "    runs = mlflow.search_runs(order_by=[\"start_time\"])\n",
    "\n",
    "    runs = runs[[\"experiment_id\", \"tags.mlflow.runName\", \n",
    "                 \"metrics.tiempo_ejecucion\", \"metrics.palabrasxsegundo\", \"metrics.keywordsxsegundo\"]]\n",
    "    \n",
    "    runs[\"Idioma\"] = runs[\"tags.mlflow.runName\"].str[:2]  \n",
    "\n",
    "    experiment_ids = runs[\"experiment_id\"].unique()\n",
    "    experiment_names = {exp_id: mlflow.get_experiment(exp_id).name for exp_id in experiment_ids}\n",
    "\n",
    "    runs[\"Nombre Modelo\"] = runs[\"experiment_id\"].map(experiment_names)\n",
    "\n",
    "    resumen = runs.groupby([\"Nombre Modelo\", \"Idioma\"]).agg({\n",
    "        \"metrics.tiempo_ejecucion\": \"mean\",\n",
    "        \"metrics.palabrasxsegundo\": \"mean\",\n",
    "        \"metrics.keywordsxsegundo\": \"mean\"\n",
    "    }).reset_index()\n",
    "\n",
    "    resumen.columns = [\"Modelo\", \"Idioma\", \"Media Tiempo (s)\", \"Media Palabras/s\", \"Media Keywords/s\"]\n",
    "    return resumen\n",
    "\n",
    "tabla_resumen = calcular_estadisticas_experimentos()\n",
    "tabla_resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610a1c5-da6f-415b-89be-0f22a830b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liberar_memoria_gpu():\n",
    "    global model, tokenizer, text_generator  \n",
    "\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "    if 'tokenizer' in globals():\n",
    "        del tokenizer\n",
    "    if 'text_generator' in globals():\n",
    "        del text_generator\n",
    "\n",
    "    # Forzar recolecci贸n de basura\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    print(\"Memoria GPU liberada correctamente.\")\n",
    "\n",
    "liberar_memoria_gpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(entorno-venv)",
   "language": "python",
   "name": "entorno-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a970d0f-82cc-49e7-88c2-1da28f792a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow; print(\"Versión MLFLOW:\", mlflow.__version__)\n",
    "import torch ; print(\"VERSIÓN TORCH:\", torch.__version__)\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "from itertools import product\n",
    "import pandas as pd ;print(\"VERSIÓN PANDAS:\", pd.__version__)\n",
    "import gc\n",
    "\n",
    "token = \"TU TOKEN DE HF\"\n",
    "login(token)\n",
    "\n",
    "# Configuración de MLflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e1fad-6304-4adf-a14e-9cc96bf7393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"deepseek-ai/Deepseek-R1-Distill-Qwen-1.5B\"\n",
    "#model_name = \"ozone-research/Chirp-01\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "#experimento = \"granite-3.2-2b-instruct\"\n",
    "experimento = \"Llama-3.2-3B-Instruct\"\n",
    "mlflow.set_experiment(experimento)\n",
    "experimento_cuantizado = experimento + \"-Cuantizado\"\n",
    "\n",
    "usar_quantizacion = False\n",
    "usar_pipeline = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750427d0-de46-4c6c-abe1-78cf7dbd7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if usar_pipeline:\n",
    "    if usar_quantizacion:\n",
    "        print(\"Usando quantización 4-bit con bitsandbytes.\")\n",
    "        mlflow.set_experiment(experimento_cuantizado)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant = True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            trust_remote_code=True, \n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) \n",
    "\n",
    "        text_generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        text_generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    print(f\"El modelo '{model_name}' es compatible con pipeline().\")\n",
    "\n",
    "else:\n",
    "    print(f\"El modelo '{model_name}' NO es compatible con pipeline(). Cargando manualmente.\")\n",
    "\n",
    "    if usar_quantizacion:\n",
    "        print(\"🔹 Usando quantización 4-bit con bitsandbytes.\")\n",
    "        mlflow.set_experiment(experimento_cuantizado)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ccc0d3f-4825-4a9b-9c77-1c6a3826125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de queries\n",
    "queries = {\n",
    "    \"en\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a useful AI assistant that extracts possible words related to the user's intent. Keep in mind that the keywords are not only within the given phrase. Present all your ideas without asking me, and delve deeply into the topic. Print only a list of keywords or ideas, without introduction, conclusion, or connectors between ideas. Use a comma separated list.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I want to search information about the wetland in Torrevieja called Parque Natural de las Lagunas de la Mata.\"}\n",
    "    ],\n",
    "    \"es\": [\n",
    "        {'role': 'system', 'content': 'Eres un útil asistente de IA que extrae posibles palabras relacionadas con la intención del usuario. Ten en cuenta que las palabras clave no solo están en la frase. Plasma todas tus ideas sin consultarme, es decir, profundiza en el tema. Imprime únicamente una lista separada por comas con las palabras clave o ideas, nada más, sin introducción ni conclusión ni conectores entre ideas.'}, \n",
    "        {'role': 'user', 'content': 'Quiero buscar información sobre un humedal en Torrevieja llamado Parque Natural de las Lagunas de la Mata.'}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e051c97c-974c-4ca0-9f94-c03647d872cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malla de parámetros\n",
    "temperaturas = [0.4, 0.7, 1.0]\n",
    "top_keys_vals = [20, 40, 60]\n",
    "top_words_vals = [0.85, 0.9, 0.95]\n",
    "num_beams_vals = [1, 2, 4]\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "param_combinations = list(product(temperaturas, top_keys_vals, top_words_vals, num_beams_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3548d-377d-4741-b70f-d2391baf5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para ejecutar el modelo y registrar en MLflow\n",
    "def ejecutar_experimento(query_name, messages, temp_val, top_k_val, top_p_val, num_beams_val, usar_pipeline, usar_quantizacion):\n",
    "    previous_runs = mlflow.search_runs(order_by=[\"start_time\"])\n",
    "    print(messages)\n",
    "\n",
    "    if \"tags.mlflow.runName\" in previous_runs.columns:\n",
    "        count = sum(previous_runs[\"tags.mlflow.runName\"].fillna(\"\").str.startswith(query_name)) + 1\n",
    "    else:\n",
    "        count = 1  \n",
    "        \n",
    "    run_name = f\"{query_name}-{count}\"  # Nombre en formato idioma-contador\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"mlflow.runName\", run_name)\n",
    "        mlflow.log_param(\"modelo\", model_name)\n",
    "        mlflow.log_param(\"query_idioma\", query_name)\n",
    "        mlflow.log_param(\"temperatura\", temp_val)\n",
    "        mlflow.log_param(\"top_k\", top_k_val)\n",
    "        mlflow.log_param(\"top_p\", top_p_val)\n",
    "        mlflow.log_param(\"num_beams\", num_beams_val)\n",
    "        mlflow.log_param(\"quantizado\", usar_quantizacion)\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        inicio = time.time()\n",
    "\n",
    "        if usar_pipeline:    \n",
    "            output = text_generator(\n",
    "                messages,\n",
    "                max_new_tokens = 500,\n",
    "                do_sample=True,\n",
    "                temperature=temp_val,\n",
    "                top_k=top_k_val,\n",
    "                top_p=top_p_val,\n",
    "                num_beams=num_beams_val,\n",
    "                repetition_penalty=1.2,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            respuesta = output[0][\"generated_text\"]\n",
    "            final = time.time() - inicio\n",
    "\n",
    "            palabras = respuesta.split(\" \")\n",
    "            cantidad_palabras = len(palabras)\n",
    "            \n",
    "            keywords = respuesta.split(\",\")\n",
    "            cantidad_keywords = len(keywords)\n",
    "            mlflow.log_metric(\"palabras\", cantidad_palabras)\n",
    "            mlflow.log_metric(\"palabrasxsegundo\", cantidad_palabras / final)\n",
    "            mlflow.log_metric(\"keywords\", cantidad_keywords)\n",
    "            mlflow.log_metric(\"keywordsxsegundo\", cantidad_keywords / final)\n",
    "        else:\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    input_ids.to(\"cuda\"),\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    max_new_tokens = 500,\n",
    "                    do_sample=True,\n",
    "                    temperature=temp_val,\n",
    "                    top_k=top_k_val,\n",
    "                    top_p=top_p_val,\n",
    "                    num_beams=num_beams_val,\n",
    "                    repetition_penalty=1.2\n",
    "                )\n",
    "    \n",
    "            respuesta = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()\n",
    "            if (query_name == \"en\"):\n",
    "                respuesta = respuesta.split(\"assistant\")[2]\n",
    "            else:\n",
    "                respuesta = respuesta.split(\"assistant\")[1]\n",
    "                \n",
    "            final = time.time() - inicio\n",
    "\n",
    "            palabras = respuesta.split(\" \")\n",
    "            cantidad_palabras = len(palabras)\n",
    "            \n",
    "            keywords = respuesta.split(\",\")\n",
    "            cantidad_keywords = len(keywords)\n",
    "\n",
    "            mlflow.log_metric(\"palabras\", cantidad_palabras)\n",
    "            mlflow.log_metric(\"palabrasxsegundo\", cantidad_palabras / final)\n",
    "            mlflow.log_metric(\"keywords\", cantidad_keywords)\n",
    "            mlflow.log_metric(\"keywordsxsegundo\", cantidad_keywords / final)\n",
    "        \n",
    "        memoria_usada = round(torch.cuda.memory_allocated() / (1024**2), 2)\n",
    "        memoria_max_usada = round(torch.cuda.max_memory_allocated() / (1024**2), 2)\n",
    "\n",
    "        mlflow.set_tag(\"respuesta\", respuesta)\n",
    "        mlflow.log_metric(\"tiempo_ejecucion\", time.time() - inicio)\n",
    "        mlflow.log_metric(\"gpu_mem_usada_MB\", memoria_usada)\n",
    "        mlflow.log_metric(\"gpu_mem_max_usada_MB\", memoria_max_usada)\n",
    "    \n",
    "        print(f\"\\n--- Respuesta ---\\n\", respuesta)\n",
    "        print(\"Tiempo de ejecución:\", time.time() - inicio)\n",
    "\n",
    "# Ejecutar experimentos en inglés y español\n",
    "for idx, (temp_val, top_k_val, top_p_val, num_beams_val) in enumerate(param_combinations):\n",
    "    ejecutar_experimento(\"en\", queries[\"en\"], temp_val, top_k_val, top_p_val, num_beams_val, usar_pipeline, usar_quantizacion)\n",
    "    ejecutar_experimento(\"es\", queries[\"es\"], temp_val, top_k_val, top_p_val, num_beams_val, usar_pipeline, usar_quantizacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af9b18-88f6-4ec0-b7a1-a65c3dad4269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Idioma</th>\n",
       "      <th>Media Tiempo (s)</th>\n",
       "      <th>Media Palabras/s</th>\n",
       "      <th>Media Keywords/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>en</td>\n",
       "      <td>11.420085</td>\n",
       "      <td>14.469464</td>\n",
       "      <td>7.973382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>es</td>\n",
       "      <td>3.275390</td>\n",
       "      <td>9.749098</td>\n",
       "      <td>7.176375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Modelo Idioma  Media Tiempo (s)  Media Palabras/s  Media Keywords/s\n",
       "0  Llama-3.2-3B     en         11.420085         14.469464          7.973382\n",
       "1  Llama-3.2-3B     es          3.275390          9.749098          7.176375"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcular_estadisticas_experimentos():\n",
    "    runs = mlflow.search_runs(order_by=[\"start_time\"])\n",
    "\n",
    "    runs = runs[[\"experiment_id\", \"tags.mlflow.runName\", \n",
    "                 \"metrics.tiempo_ejecucion\", \"metrics.palabrasxsegundo\", \"metrics.keywordsxsegundo\"]]\n",
    "    \n",
    "    runs[\"Idioma\"] = runs[\"tags.mlflow.runName\"].str[:2]  \n",
    "\n",
    "    experiment_ids = runs[\"experiment_id\"].unique()\n",
    "    experiment_names = {exp_id: mlflow.get_experiment(exp_id).name for exp_id in experiment_ids}\n",
    "\n",
    "    runs[\"Nombre Modelo\"] = runs[\"experiment_id\"].map(experiment_names)\n",
    "\n",
    "    resumen = runs.groupby([\"Nombre Modelo\", \"Idioma\"]).agg({\n",
    "        \"metrics.tiempo_ejecucion\": \"mean\",\n",
    "        \"metrics.palabrasxsegundo\": \"mean\",\n",
    "        \"metrics.keywordsxsegundo\": \"mean\"\n",
    "    }).reset_index()\n",
    "\n",
    "    resumen.columns = [\"Modelo\", \"Idioma\", \"Media Tiempo (s)\", \"Media Palabras/s\", \"Media Keywords/s\"]\n",
    "    return resumen\n",
    "\n",
    "tabla_resumen = calcular_estadisticas_experimentos()\n",
    "tabla_resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610a1c5-da6f-415b-89be-0f22a830b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liberar_memoria_gpu():\n",
    "    global model, tokenizer, text_generator  \n",
    "\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "    if 'tokenizer' in globals():\n",
    "        del tokenizer\n",
    "    if 'text_generator' in globals():\n",
    "        del text_generator\n",
    "\n",
    "    # Forzar recolección de basura\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    print(\"Memoria GPU liberada correctamente.\")\n",
    "\n",
    "liberar_memoria_gpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (entorno-venv)",
   "language": "python",
   "name": "entorno-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
